{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Language models\n",
        "N-gram\n",
        "Implement a simple bigram language model and scorer\n",
        "\n",
        "First collect counts of unigrams and bigrams\n",
        "\n",
        "Then normalise each bigram by the frequency of the first unigram\n",
        "\n",
        "This is your bigram probability\n",
        "\n",
        "Store the output in a dict, e.g. model['want']['to'] = 0.023\n",
        "\n",
        "Save the dict to a file using pickle\n",
        "\n",
        "Now write a program that scores novel sentences based on the model"
      ],
      "metadata": {
        "id": "WlOzDpfzgXwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import re\n",
        "\n"
      ],
      "metadata": {
        "id": "Sv1LVYNKelBV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(s):\n",
        "    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n",
        "    return [''] + re.sub('  *', ' ', o).strip().split(' ')"
      ],
      "metadata": {
        "id": "wPA80hKLfGPp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram Neural Network Model\n",
        "class BigramNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        super(BigramNNmodel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # compute x': concatenation of x1 and x2 embeddings\n",
        "        embeds = self.embeddings(inputs).view(\n",
        "                (-1,self.context_size * self.embedding_dim))\n",
        "        # compute h: tanh(W_1.x' + b)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # compute W_2.h\n",
        "        out = self.linear2(out)\n",
        "        # compute y: log_softmax(W_2.h)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        # return log probabilities\n",
        "        # BATCH_SIZE x len(vocab)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "g620ZNZwfKZC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    \n",
        "    training_lines = []\n",
        "    training_samples = []\n",
        "    \n",
        "    with open(\"train.txt\") as fp:\n",
        "        training_lines = [line.rstrip() for line in fp]\n",
        "    \n",
        "    vocabulary = set([''])\n",
        "    \n",
        "    for line in training_lines:\n",
        "        tokens = tokenize(line)\n",
        "        for i in tokens: vocabulary.add(i) \n",
        "        training_samples.append(tokens)\n",
        "    word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "    idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for tokens in training_samples:\n",
        "        for i in range(len(tokens) - 1): #!!!#\n",
        "            x_train.append([word2idx[tokens[i]]]) #!!!#\n",
        "            y_train.append([word2idx[tokens[i+1]]]) #!!!#\n",
        "\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_EPOCHS = 10\n",
        "\n",
        "    train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        for i, data_tensor in enumerate(train_loader):\n",
        "            context_tensor = data_tensor[:,0:1] #!!!#\n",
        "            target_tensor = data_tensor[:,1] #!!!#\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_tensor)\n",
        "            loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "            loss.backward()\n",
        "            optimiser.step()    \n",
        "\n",
        "        print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "    torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model.lm')\n",
        "\n",
        "    print('Model saved.')"
      ],
      "metadata": {
        "id": "u-34KWecfNvF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 4\n",
        "CONTEXT_SIZE = 1\n",
        "HIDDEN_DIM = 6\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFLDXc_rfd9U",
        "outputId": "5a613f81-2264-4b78-e034-f258f71abb00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 loss: 2.8793582916259766\n",
            "Epoch: 1 loss: 2.829751491546631\n",
            "Epoch: 2 loss: 2.743652820587158\n",
            "Epoch: 3 loss: 2.584360122680664\n",
            "Epoch: 4 loss: 2.3253326416015625\n",
            "Epoch: 5 loss: 1.9789296388626099\n",
            "Epoch: 6 loss: 1.598945140838623\n",
            "Epoch: 7 loss: 1.2473078966140747\n",
            "Epoch: 8 loss: 0.9583017826080322\n",
            "Epoch: 9 loss: 0.7351698279380798\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    blob = torch.load('model.lm')\n",
        "    idx2word = blob['vocab']\n",
        "    word2idx = {k: v for v, k in idx2word.items()}\n",
        "    vocabulary = set(idx2word.values())\n",
        "\n",
        "    model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "    model.load_state_dict(blob['model'])\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "\n",
        "    lines = []\n",
        "    \n",
        "    with open(\"test.txt\") as fp:\n",
        "        lines = [line.rstrip() for line in fp]\n",
        "    \n",
        "    for line in lines:\n",
        "        tokens = tokenize(line)\n",
        "        \n",
        "        x_test = []\n",
        "        y_test = []\n",
        "        for i in range(len(tokens) - 1): #!!!#\n",
        "            x_test.append([word2idx[tokens[i]]]) #!!!#\n",
        "            y_test.append([word2idx[tokens[i+1]]]) #!!!#\n",
        "        \n",
        "        x_test = np.array(x_test)\n",
        "        y_test = np.array(y_test)\n",
        "        \n",
        "        test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "        \n",
        "        total_prob = 1.0\n",
        "        for i, data_tensor in enumerate(test_loader):\n",
        "            context_tensor = data_tensor[:,0:1] #!!!#\n",
        "            target_tensor = data_tensor[:,1] #!!!#\n",
        "            log_probs = model(context_tensor)\n",
        "            probs = torch.exp(log_probs)\n",
        "            predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
        "        \n",
        "            true_label = y_test[i][0]\n",
        "            true_word = idx2word[true_label]\n",
        "        \n",
        "            prob_true = float(probs[0][true_label])\n",
        "            total_prob *= prob_true\n",
        "        \n",
        "        print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)"
      ],
      "metadata": {
        "id": "4cMwv7vjfnIK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlifzFpjhB7y",
        "outputId": "7e8e419b-e872-4044-9502-df559c962b26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.000850\t-7.070470\t ['', 'where', 'are', 'you', '?']\n",
            "0.000529\t-7.543928\t ['', 'were', 'you', 'in', 'england', '?']\n",
            "0.002621\t-5.944099\t ['', 'are', 'you', 'in', 'mexico', '?']\n",
            "0.000039\t-10.157038\t ['', 'i', 'am', 'in', 'mexico', '.']\n",
            "0.000100\t-9.211725\t ['', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing a trigram model"
      ],
      "metadata": {
        "id": "6BsSHB8uh35v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Neural Network Model\n",
        "class TrigramNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        super(TrigramNNmodel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # compute x': concatenation of x1 and x2 embeddings\n",
        "        embeds = self.embeddings(inputs).view(\n",
        "                (-1,self.context_size * self.embedding_dim))\n",
        "        # compute h: tanh(W_1.x' + b)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # compute W_2.h\n",
        "        out = self.linear2(out)\n",
        "        # compute y: log_softmax(W_2.h)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        # return log probabilities\n",
        "        # BATCH_SIZE x len(vocab)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "aK66-9dChx_P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_trigram():\n",
        "    \n",
        "    training_lines = []\n",
        "    training_samples = []\n",
        "    \n",
        "    with open(\"train.txt\") as fp:\n",
        "        training_lines = [line.rstrip() for line in fp]\n",
        "    \n",
        "    vocabulary = set([''])\n",
        "    \n",
        "    for line in training_lines:\n",
        "        tokens = tokenize(line)\n",
        "        for i in tokens: vocabulary.add(i) \n",
        "        training_samples.append(tokens)\n",
        "    word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "    idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for tokens in training_samples:\n",
        "      for i in range(len(tokens) - 2): #!!!#\n",
        "          x_train.append([word2idx[tokens[i]], word2idx[tokens[i+1]]]) #!!!#\n",
        "          y_train.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "\n",
        "\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_EPOCHS = 10\n",
        "\n",
        "    train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        for i, data_tensor in enumerate(train_loader):\n",
        "            context_tensor = data_tensor[:,0:2] #!!!#\n",
        "            target_tensor = data_tensor[:,2] #!!!#\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_tensor)\n",
        "            loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "            loss.backward()\n",
        "            optimiser.step()    \n",
        "\n",
        "        print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "    torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model_trigram.lm')\n",
        "\n",
        "    print('Model saved.')"
      ],
      "metadata": {
        "id": "zusiRZBAh_tz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 4\n",
        "CONTEXT_SIZE = 2\n",
        "HIDDEN_DIM = 6\n",
        "train_trigram()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tnaXt7JiOZr",
        "outputId": "d52fb06b-e280-471d-80b1-54b7a16b29dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 loss: 2.830050468444824\n",
            "Epoch: 1 loss: 2.5215606689453125\n",
            "Epoch: 2 loss: 2.180320978164673\n",
            "Epoch: 3 loss: 1.8046839237213135\n",
            "Epoch: 4 loss: 1.4113456010818481\n",
            "Epoch: 5 loss: 1.046945333480835\n",
            "Epoch: 6 loss: 0.756178081035614\n",
            "Epoch: 7 loss: 0.5483984351158142\n",
            "Epoch: 8 loss: 0.4094017446041107\n",
            "Epoch: 9 loss: 0.3179492950439453\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_trigram():\n",
        "    blob = torch.load('model_trigram.lm')\n",
        "    idx2word = blob['vocab']\n",
        "    word2idx = {k: v for v, k in idx2word.items()}\n",
        "    vocabulary = set(idx2word.values())\n",
        "\n",
        "    model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "    model.load_state_dict(blob['model'])\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "\n",
        "    lines = []\n",
        "    with open(\"test.txt\") as fp:\n",
        "        lines = [line.rstrip() for line in fp]\n",
        "    \n",
        "    for line in lines:\n",
        "        tokens = tokenize(line)\n",
        "        \n",
        "        x_test = []\n",
        "        y_test = []\n",
        "        for i in range(len(tokens) - 2): #!!!#\n",
        "            x_test.append([word2idx[tokens[i]], word2idx[tokens[i+1]]]) #!!!#\n",
        "            y_test.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "        \n",
        "        x_test = np.array(x_test)\n",
        "        y_test = np.array(y_test)\n",
        "        \n",
        "        test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "        \n",
        "        total_prob = 1.0\n",
        "        for i, data_tensor in enumerate(test_loader):\n",
        "            context_tensor = data_tensor[:,0:2] #!!!#\n",
        "            target_tensor = data_tensor[:,2] #!!!#\n",
        "            log_probs = model(context_tensor)\n",
        "            probs = torch.exp(log_probs)\n",
        "            predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
        "        \n",
        "            true_label = y_test[i][0]\n",
        "            true_word = idx2word[true_label]\n",
        "        \n",
        "            prob_true = float(probs[0][true_label])\n",
        "            total_prob *= prob_true\n",
        "        print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)"
      ],
      "metadata": {
        "id": "Hh50AcexiqLO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_trigram()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2KXJJpcjXV7",
        "outputId": "f3ddb2e8-2b2f-4f1e-d6ff-75e8b8863511"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.025998\t-3.649754\t ['', 'where', 'are', 'you', '?']\n",
            "0.083461\t-2.483377\t ['', 'were', 'you', 'in', 'england', '?']\n",
            "0.060088\t-2.811947\t ['', 'are', 'you', 'in', 'mexico', '?']\n",
            "0.000545\t-7.514688\t ['', 'i', 'am', 'in', 'mexico', '.']\n",
            "0.001444\t-6.540078\t ['', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
          ]
        }
      ]
    }
  ]
}